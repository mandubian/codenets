
lang_ids {
    php = 0
    python = 1
    ruby = 2
    java = 3
    go = 4
    javascript = 5
}

common_vocab_size = 10000

bert {
    hidden_size = 128
    vocab_size = ${common_vocab_size}
    intermediate_size = 512
    num_hidden_layers = 3
    num_attention_heads = 8
}

tokenizers {
    type = "TOKENIZER_TYPE"
    build_path = "./build_tokenizers/with_lang_"${tokenizers.type}
    token_files = "./build_tokenizers/token_files_"${tokenizers.type}
}

dataset {
    root_dir = ${HOME}"/workspaces/tools/CodeSearchNet/src"
    common_params {
        fraction_using_func_name=0.1
        min_len_func_name_for_query=12
        use_subtokens=False
        mark_subtoken_end=False
        code_max_num_tokens=200
        query_max_num_tokens=30
        use_bpe=True
        vocab_size=${common_vocab_size}
        pct_bpe=0.5
        vocab_count_threshold=10
        lang_ids = ${lang_ids}
        do_lowercase = true
        special_tokens = ["<unk>"]
        parallelize = true
        use_lang_weights = False
    }

    train {
        dirs = ${dataset.root_dir}"/data_dirs_train.txt"
        params = ${dataset.common_params}
    }

    val {
        dirs = ${dataset.root_dir}"/data_dirs_valid.txt"
        params = ${dataset.common_params}
    }

    test {
        dirs = ${dataset.root_dir}"/data_dirs_test.txt"
        params = ${dataset.common_params}
    }

    queries_file = ${dataset.root_dir}"/queries.csv"
}


training {
    # The name of current experiment (can have several runs)
    name = "EXPERIMENT_NAME"
    # The unique id of current run
    iteration = "UNIQUE_RUN_ID"
    # The ID used to identify the pre-built pickled files
    # using the tokenizer defined above
    tokenizer_type = "TOKENIZER_ID"
    
    # Set that to true to test your run without slow-loading train dataset
    short_circuit = false

    device = "cuda"
    # deactivate wandb & tensorboard
    wandb = true
    tensorboard = true

    model {
        # IMPORTANT: the class representing Training Context
        training_ctx_class = "codenets.codesearchnet.query_1_code_1.training_ctx.Query1Code1Ctx"
        output_size = 64
        query_encoder {
            hidden_size = ${training.model.output_size}
            vocab_size = ${common_vocab_size}
            intermediate_size = 512
            num_hidden_layers = 3
            num_attention_heads = 8
        }
        code_encoder {
            hidden_size = ${training.model.output_size}
            vocab_size = ${common_vocab_size}
            intermediate_size = 512
            num_hidden_layers = 6
            num_attention_heads = 8
        }
    }

    # Training Hyper-Parameters
    seed = 0
    lr = 0.0001
    max_grad_norm = 1.0
    min_log_interval = 50
    start_epoch = 0
    epochs = 10

    batch_size {
        train = 256
        val = 256
        test = 256
    }

    loss {
        type = "softmax_cross_entropy"
        margin = 1.0
    }

    # Paths
    pickle_path = "./pickles"
    output_dir = "./checkpoints"
    tensorboard_path = "./runs"

}